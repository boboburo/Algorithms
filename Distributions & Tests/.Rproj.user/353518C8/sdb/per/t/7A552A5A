{
    "contents" : "---\ntitle: \"Distributions\"\nauthor: \"Brian\"\ndate: \"15 August 2015\"\noutput: html_document\n---\n\n<!--\necho = show the code\nresults = 'asis' or 'hide' or 'hold' - hold till the end\nmessages: option message (FALSE hides messages in the output)\nwarnings: option warning (FALSE hides warnings in the output)\nerrors: option error (FALSE will make R stop if an error occurs; TRUE will show the error messages in the output)\ninclude = show the code-->\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(fig.width=5, fig.height=3, fig.align ='center',\n                      fig.path='Figs/',\n                      echo=TRUE, warning=FALSE, message=FALSE,\n                      results='hold',\n                      include=TRUE,\n                      cache=TRUE)\n\n# If you run into problems with cached output you can always clear the knitr cache by removing the folder named with a _cache suffix within your documentâ€™s directory.\n\n```\n\n# r,d,q,p in R\n\nR comes with a rich set of probability distributions, and four consistent ways of accessing them: r, d, q, and p. If you combine these with the name of the distribution - rnorm(), dnorm(), qnorm(), and pnorm(), for example - you can generate random numbers, find the value of the function at a specific point, produce a critical value, and calculate a p-value. \n\n### r\nWhen you want to generate random numbers, use the r prefix, as in rf() for the F distribution. Most distributions require one or more parameters to define the shape of the distribution. For the F distribution, you need the degrees of freedom for the numerator (df1) and the denominator (df2). To generate n=50 random numbers;\n\n```{r,echo=TRUE}\nrf(n=50, df1=15, df2=20)\nhist(rf(100000, df1=15, df2=20), breaks=100, col=\"gray\", xlab=\"F\", main=\"\")\n```\n\n###d\n\nThe d prefix, such as df(), is useful for finding the value of the F function, called its density, at any value of the statistic. Useful to look at the shape o fa curve also from a sequence.\n\n```{r,echo=TRUE}\ndf(x=2.3, df1=15, df2=20)\nmyF <- seq(from=0, to=6, by=0.01)\ndF <- df(x=myF, df1=15, df2=20)\nplot(myF, dF, type=\"l\", lwd=3, col=\"blue\")\n```\n\n### q\n\nThe q prefix, as in qf(), is how you obtain critical values from a probability distribution. By default, R will integrate the probability starting from the left tail. If you want to get the probability starting from the right tail, be sure to set lower.tail=FALSE. \n```{r,echo=TRUE}\nalpha = 0.05\nqf(p=alpha, df1=15, df2=20, lower.tail=FALSE)\n```\n\np\nThe p prefix, as in pf(), is how you calculate a p-value from a probability distribution. As for the q prefix, R will always integrate starting with the lower tail. If you want the right tail, specify lower.tail=FALSE. Specify the value of F for which you want a p-value, the degrees of freedom, and whether you want the left tail or the right tail.\n\n```{r}\nobservedF = 3.7\npf(q=observedF, , df1=15, df2=20, lower.tail=FALSE)\n```\n\n# Normal Distribution\n\nThe normal distribution appears in many biological and other process. It is symetrical and is formed by the *central limit theorem* Many random effects will be approximately normally distributed. So peoples height, weight etc. \n\n```{r norm1, fig.show='hold',,fig.width=10,echo=FALSE}\nlibrary(UsingR)\nlibrary(ggplot2)\ndata<-babies\n\n#Create two plots one of the actual data, and one transpose to the Standard Normal N(0,1)\n#Using ggplot\nnorm1a <- ggplot(data, aes(x=wt)) +\n  geom_histogram(colour = \"darkgreen\", fill = \"white\",aes(y = ..density..)) +\n  geom_density(colour=\"darkgreen\")+geom_density(adjust=5,colour=\"red\") +\n  xlab('Babies Weight')+ylab('density') \n\n\nnorm1a <- ggplot(data, aes(x=wt)) +\n  geom_histogram(colour = \"darkgreen\", fill = \"white\",aes(y = ..density..)) +\n  geom_density(colour=\"darkgreen\")+geom_density(adjust=5,colour=\"red\") +\n  xlab('Babies Weight')+ylab('density') +\n  ggtitle(\"Histogram of Babies Weights\") \n\n\n\ndata$wtscale<-scale(data$wt,center=TRUE,scale=TRUE)\n\n#z score density - there probably is some function for this? \nzdenz <- function (x) {\n zd<-(1/sqrt(2*pi))*exp(-1*((x^2)/2))\n return(zd)}\n\nnorm1b<-ggplot(data, aes(x=wtscale)) +\n  geom_histogram(colour = \"darkgreen\", fill = \"white\",aes(y = ..density..)) +\n  geom_density(colour=\"darkgreen\")+geom_density(adjust=5,colour=\"red\") +\n  xlab('Babies Weight')+ylab('density') +\n  ggtitle(\"Histogram Scaled N(0,1) Babies Weights \\n Z density is calculated\") +\n  geom_segment(aes(x = -1, y = 0, xend = -1, yend = zdenz(-1)),lty=2,colour=\"blue\")+\n  geom_segment(aes(x = 1, y = 0, xend = 1, yend = zdenz(1)),lty=2,colour=\"blue\")+\n   geom_segment(aes(x = -2, y = 0, xend = -2, yend = zdenz(-2)),lty=2,lwd=2,colour=\"black\")+\n  geom_segment(aes(x = 2, y = 0, xend = 2, yend = zdenz(2)),lty=2,lwd=2,colour=\"black\")\n\nlibrary(gridExtra)\ngrid.arrange(norm1a, norm1b, ncol=2)\n\n\n\n```\n\n#Chi Squared Distribution\n\nIf you sample from a normal distribution and sqaure the results you will get a $\\chi^{2}$ distribution. More formally $X \\sim \\aleph(0,1)$. Depending on the number of samples that are taken, the degrees of freedom pushes the probablity density distribution to the right until it forms a normal distribution itself.  \n\nThe distribution is additive. Therefore $\\chi^2_1=X^2_1 \\:and\\: \\chi^2_2=X^2_1+X^2_2$ etc. \n\n\n```{r chi1, echo=FALSE,fig.show='hold'}\n\n#lets do in ggplot\n\n\nx <- rchisq(100, 5)\nhist(x,prob=TRUE,xlim=c(0,35),main=\"Chi Square Distribution\")\ncurve( dchisq(x, df=1), col='green', add=TRUE)\ncurve( dchisq(x, df=2), col='red', add=TRUE)\ncurve( dchisq(x, df=3), col='blue', add=TRUE)\ncurve( dchisq(x, df=4), col='black', add=TRUE)\ncurve( dchisq(x, df=5), col='darkgreen', add=TRUE)\ncurve( dchisq(x, df=10), col='red', add=TRUE )\n\n\n#Function to sample from data Lets sample from\nchisamp<-function(degfree){\ntake <- sample(unique(data$wtscale), degfree)\ntake<-take^2\ntake<-sum(take)\nreturn(take)}\n\n\n#data frame for the results\nchisampdf <- data.frame(z= numeric(0),degf= character(0))\nfor (i in 1:5 ) {\n  results <- sapply(1:1000, function(x) { chisamp(i) + 1}) #apply the function chisamp 1000 times \n  tempdf<-data.frame(z=results,degf=paste(\"df\",i))\n  chisampdf<-rbind(chisampdf,tempdf)\n}  \n  \nchi1a<-ggplot(chisampdf, aes(z, fill = degf)) + geom_density(alpha = 0.2,adjust=5)\nchi1a<-chi1a+ggtitle(\"Chi Square Distribution of Babies Weight, smoothed\")\nchi1a\n\n```\n\n#F Distribtuion\n\nIf V 1 and V 2 are two independent random variables having the Chi-Squared distribution with m1 and m2 degrees of freedom respectively, then the following quantity follows an F distribution with m1 numerator degrees of freedom and m2 denominator degrees of freedom, i.e., (m1,m2) degrees of freedom.\n\n$$F = \\frac{V1_{m1}}{V2_{m2}} \\sim F_{(m1,m2)}$$\n\nNow on to the t-distribution and the F-distrution ratio test. \n\nF and chi-squared statistics are really the same thing in that, after a normalization, chi-squared is the limiting distribution of the F as the denominator degrees of freedom goes to infinity. The normalization is chi-squared = (numerator degrees of freedom) * F. \n\n```{r}\n\nqf(.95, df1=5, df2=2) \n\n\n#F Distribution - right probability\n1-pf(2.05,df1=2,df2=71)\n1-pf(2.05,df1=2,df2=150)\n1-pf(2.05,df1=2,df2=1500)\n1-pf(2.05,df1=2,df2=15000)\n\n#Significant value \nqf(0.95,df1=2,df2=71)\nqf(0.95,df1=2,df2=15000)\n\n\n1-pchisq(4.05, df=2) \n\nqchisq(0.1287,df=2)\n\n1-pchisq(3,2)\n\n\n\n\n\n```\n\n\n\n",
    "created" : 1439647694145.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2921454630",
    "id" : "7A552A5A",
    "lastKnownWriteTime" : 1439660711,
    "path" : "~/Desktop/GitProject/Distributions & Tests/Distributions and Tests.Rmd",
    "project_path" : "Distributions and Tests.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}